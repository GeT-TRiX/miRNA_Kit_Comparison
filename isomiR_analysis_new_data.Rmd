---
title: "isomiR_analysis"
author: "Carrie Wright"
date: "2/22/2018"
output: html_document
---

Trimming code on SRV4 /media/Backup1_/smallRNA/FullHiSeq_mismatch0/repro/samples_lanesCombined/trimmed_fastq_liberal
```{bash, eval = FALSE}
< trim_sample_list.txt parallel -P4 "/home/carrie/cutadapt -a TGGAATTCTCGGGTGCCAAGG -o $outDir/NEXT_trim1.{}.fq $inDir/NEXTFlex{}_*_R1_001.fastq.gz"

< NEXTtrim_sample_list.txt parallel -P4 "/home/carrie/cutadapt -u 4 -o NEXT_trim2.{}.fq NEXT_trim1.{}.fq"

< NEXTtrim_sample_list.txt parallel -P4 "/home/carrie/cutadapt -m 16 -u -4 -o NEXT_trimmed.{}.fq NEXT_trim2.{}.fq"

< trim_sample_list.txt parallel -P4 "/home/carrie/cutadapt -m 16 -u 3 -a AAAAAAAAAA -o  $outDir/Clontech_trimmed.{}.fq $inDir/Clontech{}_*_R1_001.fastq.gz"

< trim_sample_list.txt parallel -P4 "/home/carrie/cutadapt -m 16 -a TGGAATTCTCGGGTGCCAAGG -o $outDir/Illumina_trimmed.{}.fq $inDir/Illumina{}_*_R1_001.fastq.gz"

< trim_sample_list.txt parallel -P4 "/home/carrie/cutadapt -m 16 -a AGATCGGAAGAGCACACGTCT -o $outDir/NEB_trimmed.{}.fq $inDir/NEB{}_*_R1_001.fastq.gz"
```

UMI Script for isomiRs on SRV4

```{bash, eval =FALSE}
mkdir UMI_duplicates_rem

inDir=/media/Backup1_/smallRNA/FullHiSeq_mismatch0/repro/samples_lanesCombined/trimmed_fastq_liberal
outDir=/media/Backup1_/smallRNA/FullHiSeq_mismatch0/repro/samples_lanesCombined/trimmed_fastq_liberal/UMI_duplicates_rem

#reading every 4th line starting with line 2, get first 4 characters of sequence
awk2='NR%4==2'
< list_for_UMI.txt parallel -P4 "cat $inDir/NEXT_trim1.{}.fq | awk '$awk2' | cut -d' ' -f2 | cut -c1-4 > $outDir/first4_{}.txt"

#reading every 4th line starting with line 2, get last 4 characters of sequence
< list_for_UMI.txt parallel -P4 "cat $inDir/NEXT_trim1.{}.fq | awk '$awk2' | sed 's/^.*\(.\{4\}\)/\1/' > $outDir/last4_{}.txt"

#pasting first UMI 4 nuc. with last UMI 4 nuc.
< list_for_UMI.txt parallel -P4 "paste -d'\0' $outDir/first4_{}.txt $outDir/last4_{}.txt > $outDir/UMI_{}.txt"

#quadruple UMIs
< list_for_UMI.txt parallel -P4 "awk '{for(i=0;i<4;i++)print}' $outDir/UMI_{}.txt >$outDir/quad_UMI_{}.txt"

# add an "_" to the front of every UMI line
awk3='$0="_"$0'
< list_for_UMI.txt parallel -P4 "awk '$awk3'  $outDir/quad_UMI_{}.txt > $outDir/final_UMI_{}.txt"

# add the UMI to the fastq file identifier line
awk4='{getline p<f} (NR%4==1){$1=$1" "$2;$2=p}1'
< list_for_UMI.txt parallel -P4 "awk '$awk4' OFS= f=$outDir/final_UMI_{}.txt $inDir/NEXT_trim1.{}.fq > $outDir/NEXT_{}_UMItools_R1.fq"

#remove reads from fastq with Ns in the UMI:
#< list_for_UMI.txt parallel -P4 "sed -e '/_N\|_.*N/,+3d' $outDir/NEXT_{}_UMItools_R1.fq > $outDir/NEXT_Ns_rem_{}_UMItools_R1.fq"

#remove random 4 base pair seqs that make up the UMI from the fastq read sequence line:
< list_for_UMI.txt parallel -P4 "/home/carrie/cutadapt -u 4 -o $outDir/trim2_{}_Ns_kept_forUMI_tools.fq $outDir/NEXT_{}_UMItools_R1.fq"

< list_for_UMI.txt parallel -P4 "/home/carrie/cutadapt -m 16 -u  -4 -o $outDir/trimmed_{}_Ns_kept_forUMI_tools.fq $outDir/trim2_{}_Ns_kept_forUMI_tools.fq"


#remove space form the identifier of the fastq
< list_for_UMI.txt parallel -P4 "sed 's/ /-/' $outDir/trimmed_{}_Ns_kept_forUMI_tools.fq > $outDir/nospace_trimmed_{}_Ns_kept_forUMI_tools.fq"

#bowtie alignment
< list_for_UMI.txt parallel -P3 "/usr/bin/bowtie /media/DATA/carrie/miRge/miRge-master/miRge.seqLibs/human/mirna --fullref  -S $outDir/nospace_trimmed_{}_Ns_kept_forUMI_tools.fq $outDir/NEXT_{}_Ns_kept_readyforUMItools.sam"

#convert to bams
< list_for_UMI.txt parallel -P3 "samtools view -bS -o $outDir/NEXT_{}_Ns_kept_readyforUMItools.bam $outDir/NEXT_{}_Ns_kept_readyforUMItools.sam"

#index and sort bams
< list_for_UMI.txt parallel -P3 "samtools sort $outDir/NEXT_{}_Ns_kept_readyforUMItools.bam $outDir/NEXT_{}_Ns_kept_readyforUMItools_sorted"
< list_for_UMI.txt parallel -P3 "samtools index $outDir/NEXT_{}_Ns_kept_readyforUMItools_sorted.bam"

#UMItools
< list_for_UMI.txt parallel -P3 "umi_tools dedup --method directional -I $outDir/NEXT_{}_Ns_kept_readyforUMItools_sorted.bam -S $outDir/directional_deduped_Ns_kept_{}_UMItools.bam"


#convert deduped bam files to fastq files
<list_for_UMI.txt parallel -P3 "bam2fastx -q -Q -A -o $outDir/directional_dedupped_Ns_kept_{}_TEST.fq $outDir/directional_deduped_Ns_kept_{}_UMItools.bam"
```


miRge command on SRV2
```{bash, eval = FALSE}
#located here: carrie@srv02:~/miRge/miRge-master/isomiR_study_2_20_18
<<<<<<< HEAD
perl miRge.pl --species human --diff-isomirs --phred64 --bowtie /usr/bin/bowtie --CPU 10 --SampleFiles Clontech_trimmed.1.fq,Clontech_trimmed.2.fq,Clontech_trimmed.3.fq,Clontech_trimmed.4.fq,Clontech_trimmed.5.fq,Clontech_trimmed.6.fq,Clontech_trimmed.7.fq,Clontech_trimmed.8.fq,Clontech_trimmed.9.fq,Clontech_trimmed.10.fq,Clontech_trimmed.11.fq,Clontech_trimmed.12.fq,Clontech_trimmed.13.fq,Clontech_trimmed.14.fq,Clontech_trimmed.15.fq,Clontech_trimmed.16.fq,Clontech_trimmed.17.fq,Clontech_trimmed.18.fq,Clontech_trimmed.1_acc.fq,Clontech_trimmed.2_acc.fq,Clontech_trimmed.3_acc.fq,Illumina_trimmed.1.fq,Illumina_trimmed.2.fq,Illumina_trimmed.3.fq,Illumina_trimmed.4.fq,Illumina_trimmed.5.fq,Illumina_trimmed.6.fq,Illumina_trimmed.7.fq,Illumina_trimmed.8.fq,Illumina_trimmed.9.fq,Illumina_trimmed.1_acc.fq,Illumina_trimmed.2_acc.fq,Illumina_trimmed.3_acc.fq,NEB_trimmed.1.fq,NEB_trimmed.2.fq,NEB_trimmed.3.fq,NEB_trimmed.4.fq,NEB_trimmed.5.fq,NEB_trimmed.6.fq,NEB_trimmed.7.fq,NEB_trimmed.8.fq,NEB_trimmed.9.fq,NEB_trimmed.10.fq,NEB_trimmed.11.fq,NEB_trimmed.12.fq,NEB_trimmed.1_acc.fq,NEB_trimmed.2_acc.fq,NEB_trimmed.3_acc.fq,NEXT_trimmed.1.fq,NEXT_trimmed.2.fq,NEXT_trimmed.3.fq,NEXT_trimmed.4.fq,NEXT_trimmed.5.fq,NEXT_trimmed.6.fq,NEXT_trimmed.7.fq,NEXT_trimmed.8.fq,NEXT_trimmed.9.fq,NEXT_trimmed.10.fq,NEXT_trimmed.11.fq,NEXT_trimmed.12.fq,NEXT_trimmed.13.fq,NEXT_trimmed.14.fq,NEXT_trimmed.15.fq,NEXT_trimmed.16.fq,NEXT_trimmed.17.fq,NEXT_trimmed.18.fq,NEXT_trimmed.1_acc.fq,NEXT_trimmed.2_acc.fq,NEXT_trimmed.3_acc.fq,directional_dedupped_Ns_kept_1_TEST.fq,directional_dedupped_Ns_kept_2_TEST.fq,directional_dedupped_Ns_kept_3_TEST.fq,directional_dedupped_Ns_kept_4_TEST.fq,directional_dedupped_Ns_kept_5_TEST.fq,directional_dedupped_Ns_kept_6_TEST.fq,directional_dedupped_Ns_kept_7_TEST.fq,directional_dedupped_Ns_kept_8_TEST.fq,directional_dedupped_Ns_kept_9_TEST.fq,directional_dedupped_Ns_kept_10_TEST.fq,directional_dedupped_Ns_kept_11_TEST.fq,directional_dedupped_Ns_kept_12_TEST.fq,directional_dedupped_Ns_kept_13_TEST.fq,directional_dedupped_Ns_kept_14_TEST.fq,directional_dedupped_Ns_kept_15_TEST.fq,directional_dedupped_Ns_kept_16_TEST.fq,directional_dedupped_Ns_kept_17_TEST.fq,directional_dedupped_Ns_kept_18_TEST.fq,directional_dedupped_Ns_kept_1_acc_TEST.fq,directional_dedupped_Ns_kept_2_acc_TEST.fq,directional_dedupped_Ns_kept_3_acc_TEST.fq


=======
perl miRge.pl --species human --diff-isomirs --phred64 --bowtie /usr/bin/bowtie --CPU 10 --SampleFiles Clontech_trimmed.1.fq,Clontech_trimmed.2.fq,Clontech_trimmed.3.fq,Clontech_trimmed.4.fq,Clontech_trimmed.5.fq,Clontech_trimmed.6.fq,Clontech_trimmed.7.fq,Clontech_trimmed.8.fq,Clontech_trimmed.9.fq,Clontech_trimmed.10.fq,Clontech_trimmed.11.fq,Clontech_trimmed.12.fq,Clontech_trimmed.13.fq,Clontech_trimmed.14.fq,Clontech_trimmed.15.fq,Clontech_trimmed.16.fq,Clontech_trimmed.17.fq,Clontech_trimmed.18.fq,Clontech_trimmed.1_acc.fq,Clontech_trimmed.2_acc.fq,Clontech_trimmed.3_acc.fq,Illumina_trimmed.1.fq,Illumina_trimmed.2.fq,Illumina_trimmed.3.fq,Illumina_trimmed.4.fq,Illumina_trimmed.5.fq,Illumina_trimmed.6.fq,Illumina_trimmed.7.fq,Illumina_trimmed.8.fq,Illumina_trimmed.9.fq,Illumina_trimmed.1_acc.fq,Illumina_trimmed.2_acc.fq,Illumina_trimmed.3_acc.fq,NEB_trimmed.1.fq,NEB_trimmed.2.fq,NEB_trimmed.3.fq,NEB_trimmed.4.fq,NEB_trimmed.5.fq,NEB_trimmed.6.fq,NEB_trimmed.7.fq,NEB_trimmed.8.fq,NEB_trimmed.9.fq,NEB_trimmed.10.fq,NEB_trimmed.11.fq,NEB_trimmed.12.fq,NEB_trimmed.1_acc.fq,NEB_trimmed.2_acc.fq,NEB_trimmed.3_acc.fq,NEXT_trimmed.1.fq,NEXT_trimmed.1.fq,NEXT_trimmed.2.fq,NEXT_trimmed.3.fq,NEXT_trimmed.4.fq,NEXT_trimmed.5.fq,NEXT_trimmed.6.fq,NEXT_trimmed.7.fq,NEXT_trimmed.8.fq,NEXT_trimmed.9.fq,NEXT_trimmed.10.fq,NEXT_trimmed.11.fq,NEXT_trimmed.12.fq,NEXT_trimmed.13.fq,NEXT_trimmed.14.fq,NEXT_trimmed.15.fq,NEXT_trimmed.16.fq,NEXT_trimmed.17.fq,NEXT_trimmed.18.fq,NEXT_trimmed.1_acc.fq,NEXT_trimmed.2_acc.fq,NEXT_trimmed.3_acc.fq,directional_dedupped_Ns_kept_1_TEST.fq,directional_dedupped_Ns_kept_1_TEST.fq,directional_dedupped_Ns_kept_1_TEST.fq,directional_dedupped_Ns_kept_1_TEST.fq,directional_dedupped_Ns_kept_1_TEST.fq,directional_dedupped_Ns_kept_1_TEST.fq,directional_dedupped_Ns_kept_1_TEST.fq,directional_dedupped_Ns_kept_1_TEST.fq,directional_dedupped_Ns_kept_1_TEST.fq,directional_dedupped_Ns_kept_2_TEST.fq,directional_dedupped_Ns_kept_3_TEST.fq,directional_dedupped_Ns_kept_4_TEST.fq,directional_dedupped_Ns_kept_5_TEST.fq,directional_dedupped_Ns_kept_6_TEST.fq,directional_dedupped_Ns_kept_7_TEST.fq,directional_dedupped_Ns_kept_8_TEST.fq,directional_dedupped_Ns_kept_9_TEST.fq,directional_dedupped_Ns_kept_10_TEST.fq,directional_dedupped_Ns_kept_11_TEST.fq,directional_dedupped_Ns_kept_12_TEST.fq,directional_dedupped_Ns_kept_13_TEST.fq,directional_dedupped_Ns_kept_14_TEST.fq,directional_dedupped_Ns_kept_15_TEST.fq,directional_dedupped_Ns_kept_16_TEST.fq,directional_dedupped_Ns_kept_17_TEST.fq,directional_dedupped_Ns_kept_18_TEST.fq,directional_dedupped_Ns_kept_1_acc_TEST.fq,directional_dedupped_Ns_kept_2_acc_TEST.fq,directional_dedupped_Ns_kept_3_acc_TEST.fq


#redoing
perl miRge.pl --species human --diff-isomirs --phred64 --bowtie /usr/bin/bowtie --CPU 10 --SampleFiles Clontech_trimmed.1.fq,Clontech_trimmed.2.fq,Clontech_trimmed.3.fq,Clontech_trimmed.4.fq,Clontech_trimmed.5.fq,Clontech_trimmed.6.fq,Clontech_trimmed.7.fq,Clontech_trimmed.8.fq,Clontech_trimmed.9.fq,Clontech_trimmed.10.fq,Clontech_trimmed.11.fq,Clontech_trimmed.12.fq,Clontech_trimmed.13.fq,Clontech_trimmed.14.fq,Clontech_trimmed.15.fq,Clontech_trimmed.16.fq,Clontech_trimmed.17.fq,Clontech_trimmed.18.fq,Clontech_trimmed.1_acc.fq,Clontech_trimmed.2_acc.fq,Clontech_trimmed.3_acc.fq,Illumina_trimmed.1.fq,Illumina_trimmed.2.fq,Illumina_trimmed.3.fq,Illumina_trimmed.4.fq,Illumina_trimmed.5.fq,Illumina_trimmed.6.fq,Illumina_trimmed.7.fq,Illumina_trimmed.8.fq,Illumina_trimmed.9.fq,Illumina_trimmed.1_acc.fq,Illumina_trimmed.2_acc.fq,Illumina_trimmed.3_acc.fq,NEB_trimmed.1.fq,NEB_trimmed.2.fq,NEB_trimmed.3.fq,NEB_trimmed.4.fq,NEB_trimmed.5.fq,NEB_trimmed.6.fq,NEB_trimmed.7.fq,NEB_trimmed.8.fq,NEB_trimmed.9.fq,NEB_trimmed.10.fq,NEB_trimmed.11.fq,NEB_trimmed.12.fq,NEB_trimmed.1_acc.fq,NEB_trimmed.2_acc.fq,NEB_trimmed.3_acc.fq,NEXT_trimmed.1.fq,NEXT_trimmed.2.fq,NEXT_trimmed.3.fq,NEXT_trimmed.4.fq,NEXT_trimmed.5.fq,NEXT_trimmed.6.fq,NEXT_trimmed.7.fq,NEXT_trimmed.8.fq,NEXT_trimmed.9.fq,NEXT_trimmed.10.fq,NEXT_trimmed.11.fq,NEXT_trimmed.12.fq,NEXT_trimmed.13.fq,NEXT_trimmed.14.fq,NEXT_trimmed.15.fq,NEXT_trimmed.16.fq,NEXT_trimmed.17.fq,NEXT_trimmed.18.fq,NEXT_trimmed.1_acc.fq,NEXT_trimmed.2_acc.fq,NEXT_trimmed.3_acc.fq,directional_dedupped_Ns_kept_1_TEST.fq,directional_dedupped_Ns_kept_2_TEST.fq,directional_dedupped_Ns_kept_3_TEST.fq,directional_dedupped_Ns_kept_4_TEST.fq,directional_dedupped_Ns_kept_5_TEST.fq,directional_dedupped_Ns_kept_6_TEST.fq,directional_dedupped_Ns_kept_7_TEST.fq,directional_dedupped_Ns_kept_8_TEST.fq,directional_dedupped_Ns_kept_9_TEST.fq,directional_dedupped_Ns_kept_10_TEST.fq,directional_dedupped_Ns_kept_11_TEST.fq,directional_dedupped_Ns_kept_12_TEST.fq,directional_dedupped_Ns_kept_13_TEST.fq,directional_dedupped_Ns_kept_14_TEST.fq,directional_dedupped_Ns_kept_15_TEST.fq,directional_dedupped_Ns_kept_16_TEST.fq,directional_dedupped_Ns_kept_17_TEST.fq,directional_dedupped_Ns_kept_18_TEST.fq,directional_dedupped_Ns_kept_1_acc_TEST.fq,directional_dedupped_Ns_kept_2_acc_TEST.fq,directional_dedupped_Ns_kept_3_acc_TEST.fq

#redoing again


carrie@srv02:~/miRge/miRge-master$ perl miRge.pl --species human --diff-isomirs --phred64 --bowtie /usr/bin/bowtie --CPU 10 --SampleFiles Clontech_trimmed.1_acc.fq,Clontech_trimmed.2_acc.fq,Clontech_trimmed.3_acc.fq,Illumina_trimmed.1_acc.fq,Illumina_trimmed.2_acc.fq,Illumina_trimmed.3_acc.fq,NEB_trimmed.1_acc.fq,NEB_trimmed.2_acc.fq,NEB_trimmed.3_acc.fq,NEXT_trimmed.1_acc.fq,NEXT_trimmed.2_acc.fq,NEXT_trimmed.3_acc.fq,directional_deduped_int_trim_Ns_rem_1_acc.fq,directional_deduped_int_trim_Ns_rem_2_acc.fq,directional_deduped_int_trim_Ns_rem_3_acc.fq,Clontech_trimmed.1.fq,Clontech_trimmed.2.fq,Clontech_trimmed.3.fq,Clontech_trimmed.4.fq,Clontech_trimmed.5.fq,Clontech_trimmed.6.fq,Clontech_trimmed.7.fq,Clontech_trimmed.8.fq,Clontech_trimmed.9.fq,Clontech_trimmed.10.fq,Clontech_trimmed.11.fq,Clontech_trimmed.12.fq,Clontech_trimmed.13.fq,Clontech_trimmed.14.fq,Clontech_trimmed.15.fq,Clontech_trimmed.16.fq,Clontech_trimmed.17.fq,Clontech_trimmed.18.fq,Illumina_trimmed.1.fq,Illumina_trimmed.2.fq,Illumina_trimmed.3.fq,Illumina_trimmed.4.fq,Illumina_trimmed.5.fq,Illumina_trimmed.6.fq,Illumina_trimmed.7.fq,Illumina_trimmed.8.fq,Illumina_trimmed.9.fq,NEB_trimmed.1.fq,NEB_trimmed.2.fq,NEB_trimmed.3.fq,NEB_trimmed.4.fq,NEB_trimmed.5.fq,NEB_trimmed.6.fq,NEB_trimmed.7.fq,NEB_trimmed.8.fq,NEB_trimmed.9.fq,NEB_trimmed.10.fq,NEB_trimmed.11.fq,NEB_trimmed.12.fq,NEXT_trimmed.1.fq,NEXT_trimmed.2.fq,NEXT_trimmed.3.fq,NEXT_trimmed.4.fq,NEXT_trimmed.5.fq,NEXT_trimmed.6.fq,NEXT_trimmed.7.fq,NEXT_trimmed.8.fq,NEXT_trimmed.9.fq,NEXT_trimmed.10.fq,NEXT_trimmed.11.fq,NEXT_trimmed.12.fq,NEXT_trimmed.13.fq,NEXT_trimmed.14.fq,NEXT_trimmed.15.fq,NEXT_trimmed.16.fq,NEXT_trimmed.17.fq,NEXT_trimmed.18.fq,directional_deduped_int_trim_Ns_rem_1.fq,directional_deduped_int_trim_Ns_rem_2.fq,directional_deduped_int_trim_Ns_rem_3.fq,directional_deduped_int_trim_Ns_rem_4.fq,directional_deduped_int_trim_Ns_rem_5.fq,directional_deduped_int_trim_Ns_rem_6.fq,directional_deduped_int_trim_Ns_rem_7.fq,directional_deduped_int_trim_Ns_rem_8.fq,directional_deduped_int_trim_Ns_rem_9.fq,directional_deduped_int_trim_Ns_rem_10.fq,directional_deduped_int_trim_Ns_rem_11.fq,directional_deduped_int_trim_Ns_rem_12.fq,directional_deduped_int_trim_Ns_rem_13.fq,directional_deduped_int_trim_Ns_rem_14.fq,directional_deduped_int_trim_Ns_rem_15.fq,directional_deduped_int_trim_Ns_rem_16.fq,directional_deduped_int_trim_Ns_rem_17.fq,directional_deduped_int_trim_Ns_rem_18.fq
```


```{r}
library(here)
here()#should be Kit_comparison_project
#need subdirectory IsomiR_data with isomirs.csv inside
isomiR_Counts <-read.table(here("IsomiR_16lim_data/isomirs.csv"), header = TRUE, sep = ",", row.names = NULL)
colnames(isomiR_Counts)<-colnames(isomiR_Counts)[-(grep("row.names", colnames(isomiR_Counts)))]
isomiR_Counts<-as.data.frame(isomiR_Counts)
Entropy <- isomiR_Counts$Entropy

Pheno<- read.table(here("Pheno_repro_full_1_16_18_ns_kept.txt"), header = T)
Counts <-isomiR_Counts[3:93]
#Counts <-Counts[-grep("fq.", colnames(Counts))]# to remove extra files.....

colnames(Counts)<-gsub("directional_dedupped|directional_deduped", "Deduped", colnames(Counts))
colnames(Counts)<-gsub("NEXT_", "NEXTflex_", colnames(Counts))

#save(Counts, file =here("IsomiR_data/Counts.rda"))
#save(Entropy, file = here("IsomiR_data/Entropy.rda"))


#load(here("IsomiR_data/Entropy.rda"))
#load(here("IsomiR_data/Counts.rda"))
```

Need to convert from RPM to raw counts



```{r}
library(here)
Pheno<- read.table(here("../Pheno_repro_full_1_16_18_ns_kept"), header = TRUE)
miR_counts<-read.table(here("IsomiR_16lim_data/miR.Counts.csv"), header = TRUE, sep = ",")
#iso_RPM<-read.table(here("Brain_liberal_trimmed_Ns_kept_Data/miRge.1520646805/isomirs.csv"), header = TRUE, sep = ",")
iso_RPM <-isomiR_Counts
#fixing the columns
iso_anno <-iso_RPM[1:2]
entropy <- iso_RPM$Entropy
iso_RPM2<-iso_RPM[3:(length(colnames(iso_RPM))-2)]


library(XML)
report_url<-here("IsomiR_16lim_data/report.html")
#report_url<-here("IsomiR_data/report.html")
urltxt <- readLines(report_url)
report<-readHTMLTable(doc = report_url, header = TRUE) #need to read every third line
report <- report [-length(report)]
report <-as.data.frame(report)
report <-report[1:9]
report <-report[seq(from =1, to = nrow(report), by = 5),]
total_reads <-as.numeric(as.character(report$NULL.Total.Input.Reads))
total_miRNA<-vapply(strsplit(as.character(report$NULL.All.miRNA.Reads...Filtered.miRNA.Reads),"/"),`[`, 2, FUN.VALUE=character(1))#need to use the filtered number of miRNA reads
total_miRNA<-gsub("[[:blank:]]", "", total_miRNA)
total_miRNA<-as.numeric(as.character(total_miRNA))
total_miRNA2 <-miR_counts[1,][-1]
###convert to raw counts... use miR_counts to get total miRNA mapped number - so the miRcounts I believe includes all sequences that allign - isomir and canonical
#Total<-(miR_counts[1,])[-1] #remove miRNA column
Total<-(total_miRNA) #remove miRNA column

rep.row<-function(x,n){
   matrix(rep(x,each=n),nrow=n)
}
dimensions<-dim(iso_RPM2)
Totalmatrix<-rep.row((Total/1000000), dimensions[1])#width of isoRPM2

iso_Raw<- (iso_RPM2)*(as.numeric(Totalmatrix))
#iso_anno <-iso_RPM2_anno

#iso_Raw <-as.data.frame(sapply(iso_Raw, as.integer))

Pheno$startingAmt<-paste0("starting_amt_", Pheno$startingAmt)
###split the data by starting Amount
split_startingAmt <- list() 
for(i in Pheno$startingAmt) { 
  split_startingAmt[[i]] <- data.frame(iso_Raw[which(Pheno$startingAmt==i)])
}
###split the pheno by starting Amount
Pheno_Amt <- list() 
for(i in Pheno$startingAmt) { 
  Pheno_Amt[[i]] <- data.frame(Pheno[which(Pheno$startingAmt==i),])
}

library(edgeR)
norm_miR <-list()
for(i in unique(Pheno$startingAmt)){
d<-DGEList(counts = split_startingAmt[[i]], group = Pheno$Kit[which(Pheno$startingAmt==i)])
miR_TMM_edgeR_factors <-calcNormFactors(d, method = "TMM")
TMM <-estimateCommonDisp(miR_TMM_edgeR_factors)
norm_miR[[i]] <-data.frame(TMM$pseudo.counts)
}

#save(norm_miR, miR_counts, Pheno, file = here("IsomiR_16lim_data/iso_data.rda"))
```


#################################################################START HERE####################
#summarized data
```{r}
library(here)
load(here("IsomiR_16lim_data/iso_data.rda"))
norm_miRDF <-data.frame(norm_miR)
total_isomiRs_above10 <- colSums(norm_miRDF>10)
total_isomiR_reads <- colSums(norm_miRDF)
total_isomiR_reads <-data.frame(total_isomiR_reads)
total_isomiRs_above10<-data.frame(total_isomiRs_above10)
total_isomiR<-cbind(total_isomiR_reads, total_isomiRs_above10)
total_miRNA <-miR_counts[1,][-1]

report_url<-here("IsomiR_16lim_data/report.html")
urltxt <- readLines(report_url)
report<-readHTMLTable(doc = report_url, header = TRUE) #need to read every third line
report <- report [-length(report)]
report <-as.data.frame(report)
report <-report[1:9]
report <-report[seq(from =1, to = nrow(report), by = 5),]
total_reads <-as.numeric(as.character(report$NULL.Total.Input.Reads))


summarized_Data <- data.frame(total_miRNA=t(as.vector(total_miRNA)), report$NULL.Total.Input.Reads)
colnames(summarized_Data)[1] <- c("total_miRNA_reads")
summarized_Data$percentmiRNA <- (as.numeric(summarized_Data$total_miRNA_reads)/as.numeric(as.character(summarized_Data$report.NULL.Total.Input.Reads)))*100



```



```{r}
load(here("IsomiR_data_new_incomplete/iso_data.rda"))

split_norm_Amt <- list() 
for(i in names(norm_miR)) {
  for(kit in unique(Pheno_Amt[[i]][3]$Kit))# take the third list from each respective amount selected from norm_miR names - the third is the kit - so for each kit in the list of kits for a given amount...
  split_norm_Amt[[i]][[kit]] <-data.frame(norm_miR[[i]][which(Pheno_Amt[[i]][3] == kit)]) # take only the values that correspond to that kit for that amount
}


library(genefilter)

poverafun <- genefilter::pOverA(p = 1, A = 10)#at least 100 normalized reads in all samples of the set... 
ffun <- filterfun(poverafun)
genefilt_fun<- function(x){genefilter(x, ffun)}

thresh <-list()
for(amt in names(split_norm_Amt)){
thresh[[amt]]<-lapply(split_norm_Amt[[amt]], genefilt_fun)}

test <-split_norm_Amt$starting_amt_1000$Clontech[thresh$starting_amt_1000$Clontech,]

split_amt_thresh <-list()
for(amt in names(split_norm_Amt)){
for(kit in names(split_norm_Amt[[amt]])){
 split_amt_thresh[[amt]][[kit]]<- split_norm_Amt[[amt]][[kit]][thresh[[amt]][[kit]],]}
}


#detected miRNAs

head(iso_anno[rownames(split_amt_thresh$starting_amt_100$Clontech),])
get_miRNAs <- function(x) {rownames(as.data.frame(x))}
miRNA_det_100<-lapply(split_amt_thresh$starting_amt_100,get_miRNAs)
lengths<-lapply(miRNA_det_100, length)
lengths<- paste0(lengths, "\n")
names(miRNA_det_100) <-paste0(names(miRNA_det_100), "\n")
names(miRNA_det_100) <-paste0(names(miRNA_det_100), lengths)

miRNA_det_250<-lapply(split_amt_thresh$starting_amt_250,get_miRNAs)
lengths<-lapply(miRNA_det_250, length)
names(miRNA_det_250) <-paste0(names(miRNA_det_250), "\n")
names(miRNA_det_250) <-paste0(names(miRNA_det_250), lengths)

miRNA_det_500<-lapply(split_amt_thresh$starting_amt_500,get_miRNAs)
lengths<-lapply(miRNA_det_500, length)
names(miRNA_det_500) <-paste0(names(miRNA_det_500), "\n")
names(miRNA_det_500) <-paste0(names(miRNA_det_500), lengths)

miRNA_det_1000<-lapply(split_amt_thresh$starting_amt_1000,get_miRNAs)
lengths<-lapply(miRNA_det_1000, length)
names(miRNA_det_1000) <-paste0(names(miRNA_det_1000), "\n")
names(miRNA_det_1000) <-paste0(names(miRNA_det_1000), lengths)

miRNA_det_1500<-lapply(split_amt_thresh$starting_amt_1500,get_miRNAs)
lengths<-lapply(miRNA_det_1500, length)
names(miRNA_det_1500) <-paste0(names(miRNA_det_1500), "\n")
names(miRNA_det_1500) <-paste0(names(miRNA_det_1500), lengths)

miRNA_det_2000<-lapply(split_amt_thresh$starting_amt_2000,get_miRNAs)
lengths<-lapply(miRNA_det_2000, length)
names(miRNA_det_2000) <-paste0(names(miRNA_det_2000), "\n")
names(miRNA_det_2000) <-paste0(names(miRNA_det_2000), lengths)

library(VennDiagram)
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

n = 5
cols = gg_color_hue(n)
cols <-c(cols[1], cols[3:5])

vp_100 <- venn.diagram(miRNA_det_100, fill = c(cols), alpha = 0.5, filename = NULL, margin = 0.2, cat.dist = c(.25,.25,.15, .15))
vp_250 <- venn.diagram(miRNA_det_250, fill = c(cols), alpha = 0.5, filename = NULL, margin = 0.2, cat.dist = c(.25,.25,.15, .15))
vp_500 <- venn.diagram(miRNA_det_500, fill = c(cols), alpha = 0.5, filename = NULL, margin = 0.2, cat.dist = c(.25,.25,.15, .15))

cols = gg_color_hue(n)
vp_1000 <- venn.diagram(miRNA_det_1000, fill = c(cols), alpha = 0.5, filename = NULL, margin = 0.2, cat.dist = c(.25,.25,.25, .25, .35))

cols = gg_color_hue(n)
cols <-c(cols[1:2], cols[4:5])
vp_1500 <- venn.diagram(miRNA_det_1500, fill = c(cols), alpha = 0.5, filename = NULL, margin = 0.2, cat.dist = c(.25,.25,.15, .15))
vp_2000 <- venn.diagram(miRNA_det_2000, fill = c(cols), alpha = 0.5, filename = NULL, margin = 0.2, cat.dist = c(.25,.25,.15, .15))

```
grid.draw(vp_1000);




=======
>>>>>>> b7bb0ed04bcb3658c273201b6af02e8a95fddf50
```{r, eval  = FALSE}

###Will output two additional files, isomirs.csv and isomirs.samples.csv. The first file has the entropy of each isomir as compared with their canonical miRNAs - % of time noncanonical - higher number means less canonical more isomirs. The second file contains the entropy of each miRNA across all isomirs and determines the % of miRNA reads that are canonical. This is done by dividing the sum of all non-edited, but length variable miRNA reads against the sum of these reads plus all isomiR (RNA-edited) reads. This can be used to flag miRNAs that are predominately ####non-canonical isomiRs and may represent sequencing errors. For example, if a miRNA is only 5% canonical, that would be worrisome and suggest a possible misidentification.

#I believe the isomirs.csv file contains one column of entropy - the one called entropy for each isomiRNA (as compared with the canonical) and then RPMs... as the values are not whole numbers... then isomirs.samples.csv contains the % of the time that a miRNA is canonical and the entropy for each isomiRNA relatve to other isomiRs for each sample pr maybe its the percent of the time that an isomiRNA is a canonical type of isomir???meaning just length variable??


# I think Entropy in isomirs.samples.csv (but this maybe the percent canonical isomiR type) = sum of all length isomir reads / sum of all types of isomir (length and RNA-edited) reads - thus if too low this is bad... seems to range to about 50% or .5

#sounds like low entropy in the isomirs.samples.csv is bad - means mostly edited-- could be sequencing error - check canonical percentage too if it is low then probably sequencing error or maybe this meens the 

iso_RPM<-read.table(here("isomiR_data_extended/isomirs.csv"), header = T, sep = ",", row.names=NULL)
iso_RPM <-iso_RPM[-grep("fq.", colnames(iso_RPM))]# to remove extra files.....

isosamples<- read.table(here("isomiR_data_extended/isomirs.samples.csv"), header = T, sep = ",", row.names=NULL)
isosamples <-isosamples[-grep("RPM.", colnames(isosamples))]# to remove extra files.....

<<<<<<< HEAD
miR_counts <- read.table(here("isomiR_data/miR.Counts.csv"),header = T, sep = ",")
=======
miR_counts <- read.table(here("isomiR_data_extended/miR.Counts.csv"),header = T, sep = ",")
>>>>>>> b7bb0ed04bcb3658c273201b6af02e8a95fddf50
miR_counts <-miR_counts[-grep("fq.", colnames(miR_counts))]# to remove extra files.....

#fixing the columns
iso_data<-iso_RPM[1:(length(colnames(iso_RPM))-1)]
colnames(iso_data)<-colnames(iso_RPM)[2:(length(colnames(iso_RPM)))]
iso_RPM2 <- iso_data[,4:length(colnames(iso_data))-1]
iso_RPM2_anno<-iso_data[1:2]

#fixing the colnames
iso_samp<-isosamples[1:(length(colnames(isosamples))-1)]
colnames(iso_samp)<-colnames(isosamples)[2:length(colnames(isosamples))]
#getting just the raw info
index<-grep("RPM", colnames(iso_samp))
iso_samp_filt<-iso_samp[-index]
rownames(iso_samp_filt)<- iso_samp$miRNA
iso_samp_filt<-iso_samp_filt[-1]

topIsomiR_entropy <-  iso_samp_filt[grep("isomir", colnames(iso_samp_filt))]
Canonical <- iso_samp_filt[grep("Canonical", colnames(iso_samp_filt))]


###convert to raw counts... use miR_counts to get total miRNA mapped number - so the miRcounts I believe includes all sequences that allign - isomir and canonical
Total<-(miR_counts[1,])[-1] #remove miRNA column
rep.row<-function(x,n){
   matrix(rep(x,each=n),nrow=n)
}
dim(iso_RPM2)
Totalmatrix<-rep.row((Total/1000000), 4430447)#width of isoRPM2

iso_Raw<- (iso_RPM2)*(as.numeric(Totalmatrix))
#save(iso_Raw, file = here("isomiR_data_extended/iso_Raw.rda"))
iso_anno <-iso_RPM2_anno
<<<<<<< HEAD
#save(iso_anno, file = here("isomiR_data_extended/iso_anno.rda"))
=======
save(iso_anno, file = here("isomiR_data_extended/iso_anno.rda"))
>>>>>>> b7bb0ed04bcb3658c273201b6af02e8a95fddf50

##########

#save(Canonical, file = here("isomiR_data_extended/Canonicalpercent.rda"))
#save(topIsomiR_entropy, file = here("isomiR_data_extended/topIsomiR_entropy.rda"))

```


#####start here for faster analysis

```{r}
load(here("isomiR_data_extended/iso_Raw.rda"))
load(here("isomiR_data_extended/iso_anno.rda"))
Pheno<- read.table(here("IsomiR_data/IsoPheno.txt"), header = T)
identical(colnames(iso_Raw), as.character(Pheno$File))
miR_counts <- read.table(here("isomiR_data_extended/miR.Counts.csv"),header = T, sep = ",")
miR_counts <-miR_counts[-grep("fq.", colnames(miR_counts))]# to remove extra files.....
miR_counts <-miR_counts[-grep("acc", colnames(miR_counts))]# to remove second batch of 1000ng files.....
Total_counts <-miR_counts[1,]
miR_counts <-miR_counts[-1,]
miR_counts <-miR_counts[-1]
#remove second batch of 1000ng starting input
iso_Raw <- iso_Raw[-grep("acc", colnames(iso_Raw))]
Pheno <- Pheno [-grep("acc", Pheno$File),]
```


### comparison of error for each kit for given starting amount
##Split the data
```{r}

Pheno$startingAmt<-paste0("starting_amt_", Pheno$startingAmt)
###split the data by starting Amount
split_startingAmt <- list() 
for(i in Pheno$startingAmt) { 
  split_startingAmt[[i]] <- data.frame(iso_Raw[which(Pheno$startingAmt==i)])
}
###split the pheno by starting Amount
Pheno_Amt <- list() 
for(i in Pheno$startingAmt) { 
  Pheno_Amt[[i]] <- data.frame(Pheno[which(Pheno$startingAmt==i),])
}

Can_split_startingAmt <- list() 
for(i in Pheno$startingAmt) { 
  Can_split_startingAmt[[i]] <- data.frame(miR_counts[which(Pheno$startingAmt==i)])
}
```
###TMM Normalization

Normalization by kit for each starting amount - to allow tests to compare kits at a given starting amt

Later will do normalization by starting amount for each kit  individually to compare how consistent the data is between starting amounts for each kit
```{r, eval= TRUE, echo =FALSE}
#library(tweeDEseq)
#miR_1000_TMM<-data.frame(normalizeCounts(miR_1000_raw)
#dim(norm_miR_1000)
#or
library(edgeR)

norm_miR <-list()
for(i in unique(Pheno$startingAmt)){
d<-DGEList(counts = split_startingAmt[[i]], group = Pheno$Kit[which(Pheno$startingAmt==i)])
miR_TMM_edgeR_factors <-calcNormFactors(d, method = "TMM")
TMM <-estimateCommonDisp(miR_TMM_edgeR_factors)
norm_miR[[i]] <-data.frame(TMM$pseudo.counts)
}

#str(norm_miR)

Can_norm_miR <-list()
for(i in unique(Pheno$startingAmt)){
d<-DGEList(counts =Can_split_startingAmt[[i]], group = Pheno$Kit[which(Pheno$startingAmt==i)])
miR_TMM_edgeR_factors <-calcNormFactors(d, method = "TMM")
TMM <-estimateCommonDisp(miR_TMM_edgeR_factors)
Can_norm_miR[[i]] <-data.frame(TMM$pseudo.counts)
}

```

Now split normalized data by kit for each amount
```{r}

split_norm_Amt <- list() 
for(i in names(norm_miR)) {
  for(kit in unique(Pheno_Amt[[i]][3]$Kit))# take the third list from each respective amount selected from norm_miR names - the third is the kit - so for each kit in the list of kits for a given amount...
  split_norm_Amt[[i]][[kit]] <-data.frame(norm_miR[[i]][which(Pheno_Amt[[i]][3] == kit)]) # take only the values that correspond to that kit for that amount
}

Can_split_norm_Amt <- list() 
for(i in names(Can_norm_miR)) {
  for(kit in unique(Pheno_Amt[[i]][3]$Kit))# take the third list from each respective amount selected from norm_miR names - the third is the kit - so for each kit in the list of kits for a given amount...
  Can_split_norm_Amt[[i]][[kit]] <-data.frame(Can_norm_miR[[i]][which(Pheno_Amt[[i]][3] == kit)]) # take only the values that correspond to that kit for that amount
}
#str(split_norm_Amt)
```




###Filter the data across triplicates for given kit at a given amount
```{r}
###genefilter
library(genefilter)

poverafun <- genefilter::pOverA(p = 1, A = 10)#at least 100 normalized reads in all samples of the set... 
ffun <- filterfun(poverafun)
genefilt_fun<- function(x){genefilter(x, ffun)}

thresh <-list()
<<<<<<< HEAD
for(amt in names(split_norm_Amt)){
thresh[[amt]]<-lapply(split_norm_Amt[[amt]], genefilt_fun)}

Can_thresh <-list()
for(amt in names(Can_split_norm_Amt)){
Can_thresh[[amt]]<-lapply(Can_split_norm_Amt[[amt]], genefilt_fun)}
=======
for(amt in names(split_norm_Amt))
thresh[[amt]]<-lapply(split_norm_Amt[[amt]], genefilt_fun)

Can_thresh <-list()
for(amt in names(Can_split_norm_Amt))
Can_thresh[[amt]]<-lapply(Can_split_norm_Amt[[amt]], genefilt_fun)
>>>>>>> b7bb0ed04bcb3658c273201b6af02e8a95fddf50


test <-split_norm_Amt$starting_amt_1000$Clontech[thresh$starting_amt_1000$Clontech,]

split_amt_thresh <-list()
for(amt in names(split_norm_Amt)){
for(kit in names(split_norm_Amt[[amt]])){
 split_amt_thresh[[amt]][[kit]]<- split_norm_Amt[[amt]][[kit]][thresh[[amt]][[kit]],]}
}

Can_split_amt_thresh <-list()
for(amt in names(Can_split_norm_Amt)){
for(kit in names(Can_split_norm_Amt[[amt]])){
 Can_split_amt_thresh[[amt]][[kit]]<- Can_split_norm_Amt[[amt]][[kit]][Can_thresh[[amt]][[kit]],]}
}
```

#detected miRNAs
```{r}
head(iso_anno[rownames(split_amt_thresh$starting_amt_100$Clontech),])
get_miRNAs <- function(x) {rownames(as.data.frame(x))}
miRNA_det_100<-lapply(split_amt_thresh$starting_amt_100,get_miRNAs)
lengths<-lapply(miRNA_det_100, length)
lengths<- paste0(lengths, "\n")
names(miRNA_det_100) <-paste0(names(miRNA_det_100), "\n")
names(miRNA_det_100) <-paste0(names(miRNA_det_100), lengths)

miRNA_det_250<-lapply(split_amt_thresh$starting_amt_250,get_miRNAs)
lengths<-lapply(miRNA_det_250, length)
names(miRNA_det_250) <-paste0(names(miRNA_det_250), "\n")
names(miRNA_det_250) <-paste0(names(miRNA_det_250), lengths)

miRNA_det_500<-lapply(split_amt_thresh$starting_amt_500,get_miRNAs)
lengths<-lapply(miRNA_det_500, length)
names(miRNA_det_500) <-paste0(names(miRNA_det_500), "\n")
names(miRNA_det_500) <-paste0(names(miRNA_det_500), lengths)

miRNA_det_1000<-lapply(split_amt_thresh$starting_amt_1000,get_miRNAs)
lengths<-lapply(miRNA_det_1000, length)
names(miRNA_det_1000) <-paste0(names(miRNA_det_1000), "\n")
names(miRNA_det_1000) <-paste0(names(miRNA_det_1000), lengths)

miRNA_det_1500<-lapply(split_amt_thresh$starting_amt_1500,get_miRNAs)
lengths<-lapply(miRNA_det_1500, length)
names(miRNA_det_1500) <-paste0(names(miRNA_det_1500), "\n")
names(miRNA_det_1500) <-paste0(names(miRNA_det_1500), lengths)

miRNA_det_2000<-lapply(split_amt_thresh$starting_amt_2000,get_miRNAs)
lengths<-lapply(miRNA_det_2000, length)
names(miRNA_det_2000) <-paste0(names(miRNA_det_2000), "\n")
names(miRNA_det_2000) <-paste0(names(miRNA_det_2000), lengths)
```

#detected miRNAs Can
```{r}
Can_miRNA_det_100<-lapply(Can_split_amt_thresh$starting_amt_100,get_miRNAs)
lengths<-lapply(Can_miRNA_det_100, length)
lengths<- paste0(lengths, "\n")
names(Can_miRNA_det_100) <-paste0(names(Can_miRNA_det_100), "\n")
names(Can_miRNA_det_100) <-paste0(names(Can_miRNA_det_100), lengths)

Can_miRNA_det_250<-lapply(Can_split_amt_thresh$starting_amt_250,get_miRNAs)
lengths<-lapply(Can_miRNA_det_250, length)
names(Can_miRNA_det_250) <-paste0(names(Can_miRNA_det_250), "\n")
names(Can_miRNA_det_250) <-paste0(names(Can_miRNA_det_250), lengths)

Can_miRNA_det_500<-lapply(Can_split_amt_thresh$starting_amt_500,get_miRNAs)
lengths<-lapply(Can_miRNA_det_500, length)
names(Can_miRNA_det_500) <-paste0(names(Can_miRNA_det_500), "\n")
names(Can_miRNA_det_500) <-paste0(names(Can_miRNA_det_500), lengths)

Can_miRNA_det_1000<-lapply(Can_split_amt_thresh$starting_amt_1000,get_miRNAs)
lengths<-lapply(Can_miRNA_det_1000, length)
names(Can_miRNA_det_1000) <-paste0(names(Can_miRNA_det_1000), "\n")
names(Can_miRNA_det_1000) <-paste0(names(Can_miRNA_det_1000), lengths)

Can_miRNA_det_1500<-lapply(Can_split_amt_thresh$starting_amt_1500,get_miRNAs)
lengths<-lapply(Can_miRNA_det_1500, length)
names(Can_miRNA_det_1500) <-paste0(names(Can_miRNA_det_1500), "\n")
names(Can_miRNA_det_1500) <-paste0(names(Can_miRNA_det_1500), lengths)

Can_miRNA_det_2000<-lapply(Can_split_amt_thresh$starting_amt_2000,get_miRNAs)
lengths<-lapply(Can_miRNA_det_2000, length)
names(Can_miRNA_det_2000) <-paste0(names(Can_miRNA_det_2000), "\n")
names(Can_miRNA_det_2000) <-paste0(names(Can_miRNA_det_2000), lengths)
```

VennDiagram
```{r}
library(VennDiagram)
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

n = 5
cols = gg_color_hue(n)
cols <-c(cols[1], cols[3:5])

vp_100 <- venn.diagram(miRNA_det_100, fill = c(cols), alpha = 0.5, filename = NULL, margin = 0.2, cat.dist = c(.25,.25,.15, .15))
vp_250 <- venn.diagram(miRNA_det_250, fill = c(cols), alpha = 0.5, filename = NULL, margin = 0.2, cat.dist = c(.25,.25,.15, .15))
vp_500 <- venn.diagram(miRNA_det_500, fill = c(cols), alpha = 0.5, filename = NULL, margin = 0.2, cat.dist = c(.25,.25,.15, .15))
vp_1000 <- venn.diagram(miRNA_det_500, fill = c(cols), alpha = 0.5, filename = NULL, margin = 0.2, cat.dist = c(.25,.25,.15, .15))
vp_1500 <- venn.diagram(miRNA_det_1500, fill = c(cols), alpha = 0.5, filename = NULL, margin = 0.2, cat.dist = c(.25,.25,.15, .15))
vp_2000 <- venn.diagram(miRNA_det_2000, fill = c(cols), alpha = 0.5, filename = NULL, margin = 0.2, cat.dist = c(.25,.25,.15, .15))


vp_Can100 <- venn.diagram(Can_miRNA_det_100, fill = c(cols), alpha = 0.5, filename = NULL, margin = 0.2, cat.dist = c(.25,.25,.15, .15))
vp_Can250 <- venn.diagram(Can_miRNA_det_250, fill = c(cols), alpha = 0.5, filename = NULL, margin = 0.2, cat.dist = c(.25,.25,.15, .15))
vp_Can500 <- venn.diagram(Can_miRNA_det_500, fill = c(cols), alpha = 0.5, filename = NULL, margin = 0.2, cat.dist = c(.25,.25,.15, .15))
vp_Can1000 <- venn.diagram(Can_miRNA_det_500, fill = c(cols), alpha = 0.5, filename = NULL, margin = 0.2, cat.dist = c(.25,.25,.15, .15))
vp_Can1500 <- venn.diagram(Can_miRNA_det_1500, fill = c(cols), alpha = 0.5, filename = NULL, margin = 0.2, cat.dist = c(.25,.25,.15, .15))
vp_Can2000 <- venn.diagram(Can_miRNA_det_2000, fill = c(cols), alpha = 0.5, filename = NULL, margin = 0.2, cat.dist = c(.25,.25,.15, .15))


```

grid.draw(vp_1000);


Split total miRNA reads

```{r}

Total_counts<-Total_counts[-1]
###split the total counts by amount
split_total<- list() 
for(i in Pheno$TriplicateGroup) { 
 split_total[[i]] <- rowMeans(Total_counts[which(Pheno$TriplicateGroup==i)])
}
split_total<-data.frame(split_total)
split_total_miRNA<-split_total
boxplot(split_total_miRNA)

```

report stats
```{r}
library(XML)
<<<<<<< HEAD
report_url<-here("IsomiR_data_new_incomplete/report.html")
=======
report_url<-here("isomiR_data/report.html")
>>>>>>> b7bb0ed04bcb3658c273201b6af02e8a95fddf50
urltxt <- readLines(report_url)
report<-readHTMLTable(doc = report_url, header = TRUE) #need to read every third line
report <- report [-length(report)]
report <-as.data.frame(report)
report <-report[1:9]
report <-report[seq(from =1, to = nrow(report), by = 5),]
total_reads <-as.numeric(as.character(report$NULL.Total.Input.Reads))

split_total<- list() 
for(i in Pheno$TriplicateGroup) { 
 split_total[[i]] <- mean(total_reads[which(Pheno$TriplicateGroup==i)])
}
split_total<-data.frame(split_total)
boxplot(split_total)

<<<<<<< HEAD
split_total_miRNA<- list() 
for(i in Pheno$TriplicateGroup) { 
 split_total_miRNA[[i]] <- mean(total_miRNA[which(Pheno$TriplicateGroup==i)])
}
split_total_miRNA<-data.frame(split_total_miRNA)
                        
=======

>>>>>>> b7bb0ed04bcb3658c273201b6af02e8a95fddf50
boxplot(log2(split_total))
boxplot(log2(split_total_miRNA))

total_reads_final<-t(split_total)
total_miRNA_final<-t(split_total_miRNA)
totals <-data.frame(reads =total_reads_final, miRNAreads = total_miRNA_final)
<<<<<<< HEAD
####somehow figure out how to get lengths of isomiRS so we can see if they correlate with raw read count or miRNA total count 
=======
####somehow figure out how to get lengths of isomiRS so we can see if they correlate with raw read count or miRNA total count
>>>>>>> b7bb0ed04bcb3658c273201b6af02e8a95fddf50
get_lengths <-function(x){length(rownames(x))}
lapply(split_amt_thresh$starting_amt_starting_amt_starting_amt_100, get_lengths)
split_iso<- list() 
for(i in Pheno$startingAmt) { 
  for(kit in Pheno$Kit){
 split_iso[[i]][[kit]] <- mean(split_amt_thresh[which(Pheno$startingAmt==i & Pheno$Kit == kit)])
}}
split_total<-data.frame(split_total)
boxplot(split_total)



```

























###TMM Normalization

Normalization by kit for each starting amount - to allow tests to compare kits

```{r, eval= TRUE, echo =FALSE}
library(edgeR)

norm_isomiR <-list()
d<-DGEList(counts = iso_Raw, group = Pheno$Kit)
miR_TMM_edgeR_factors <-calcNormFactors(d, method = "TMM")
TMM <-estimateCommonDisp(miR_TMM_edgeR_factors)
norm_isomiR[[i]] <-data.frame(TMM$pseudo.counts)


```

Now split normalized data by kit for each amount
```{r}

split_norm_Amt <- list() 
for(i in names(norm_miR)) {
  for(kit in unique(Pheno_Amt[[i]][3]$Kit))# take the third list from each respective amount selected from norm_miR names - the third is the kit - so for each kit in the list of kits for a given amount...
  split_norm_Amt[[i]][[kit]] <-data.frame(norm_miR[[i]][which(Pheno_Amt[[i]][3] == kit)]) # take only the values that correspond to that kit for that amount
}
#str(split_norm_Amt)
```


################Analysis starts here#########################

```{r}
library(here)
here()#should be Kit_comparison_project
#need subdirectory IsomiR_data with isomirs.csv inside
#isomiR_Counts <-read.table(here("IsomiR_data/isomirs.csv"), header = TRUE, sep = ",", row.names = NULL)
#colnames(isomiR_Counts)<-colnames(isomiR_Counts)[-(grep("row.names", colnames(isomiR_Counts)))]
#isomiR_Counts<-as.data.frame(isomiR_Counts)
#Entropy <- isomiR_Counts$Entropy
#save(isomiR_Counts, file =here("IsomiR_data/isomiR_Counts.rda"))
Pheno<- read.table(here("Pheno_repro_full_1_16_18_ns_kept.txt"), header = T)
#Counts <-isomiR_Counts[3:101]
#Counts <-Counts[-grep("fq.", colnames(Counts))]

#colnames(Counts)<-gsub("directional_dedupped|directional_deduped", "Deduped", colnames(Counts))
#colnames(Counts)<-gsub("NEXT_", "NEXTflex_", colnames(Counts))

#save(Counts, file =here("IsomiR_data/Counts.rda"))
#save(Entropy, file = here("IsomiR_data/Entropy.rda"))

Pheno<-vapply(strsplit(names(Counts),"_"), `[`, 1, FUN.VALUE=character(1))
load(here("IsomiR_data/Entropy.rda"))
load(here("IsomiR_data/Counts.rda"))
Pheno_simple <- read.table("Pheno_simple.txt", header = TRUE)

#remove 2nd batch of 1000
Counts_clean <- Counts[-grep("2", Pheno_simple$Batch),]# can't do on my laptop

```

###TMM Normalization

Normalization by kit for each starting amount - to allow tests to compare kits

```{r, eval= TRUE, echo =FALSE}
library(edgeR)

norm_isomiR <-list()
d<-DGEList(counts = Counts, group = Pheno_simple$Kit)
miR_TMM_edgeR_factors <-calcNormFactors(d, method = "TMM")
TMM <-estimateCommonDisp(miR_TMM_edgeR_factors)
norm_isomiR[[i]] <-data.frame(TMM$pseudo.counts)
}

for(i in unique(Pheno_simple$Kit)){
d<-DGEList(counts = Counts[grep(i, colnames(Counts))], group = Pheno_simple$Kit[which(Pheno_simple$Kit==i)])
miR_TMM_edgeR_factors <-calcNormFactors(d, method = "TMM")
TMM <-estimateCommonDisp(miR_TMM_edgeR_factors)
norm_isomiR[[i]] <-data.frame(TMM$pseudo.counts)
}

NEXTflex <- Counts[grep(i, colnames())]

#str(norm_isomiR)

```

Now split normalized data by kit for each amount
```{r}

split_norm_Kit <- list() 
for(i in names(norm_miR)) {
  for(kit in unique(Pheno_Amt[[i]][3]$Kit))# take the third list from each respective amount selected from norm_miR names - the third is the kit - so for each kit in the list of kits for a given amount...
  split_norm_Kit[[i]][[kit]] <-data.frame(norm_isomiR[[i]][which(Pheno_Amt[[i]][3] == kit)]) # take only the values that correspond to that kit for that amount
}
#str(split_norm_Amt)
```

results in nice format
```{r}

results<-read.csv(here("../../kit_comp_isomir_results.csv"), header = TRUE)
results <-results[1:5,]
meltedresult<-melt(results)
```
